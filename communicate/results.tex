
To assess \mine's effectiveness, we implemented both it and comparison algorithms in C++ and Python. All of our code and raw results are publicly available on the \mine website.\footnote{https://github.com/dblalock/sprintz} This website also contains experiments on additional datasets, as well as thorough documentation of both our code and experimental setups. All experiments use a single thread on a 2013 Macbook Pro with a 2.6GHz Intel Core i7-4960HQ processor.

All reported timings and throughputs are the best of 5 runs, averaged over 10 trials (i.e., the code is executed 50 times). We use the best in each trial, rather than average, since this is standard practice in performance benchmarking.

% ------------------------------------------------
\subsection{Datasets}
% ------------------------------------------------

For assessing accuracy, we use several datasets widely used to benchmark Multi-Codebook Quantization (MCQ) algorithms:
\begin{itemize}[leftmargin=4mm]
% \item \textbf{Batman} \cite{batman} --- Also known as the Caped Crusader. Don't ask what compression means here, because I couldn't tell you.
\item \textbf{UCR} \cite{ucrTimeSeries} --- We concatenated the first 100 examples from each of the \%d time series datasets in the UCR Time Series Archive \cite{ucrTimeSeries} to form \%d longer datasets. Before concatenating, we subtracted off the mean from each example and interpolated one sample between its end and the start of the next example to avoid sudden jumps. This processing makes the datasets in some sense synthetic, but the result is an easy-to-reproduce benchmark incorporating time series from dozens of domains. We report aggregate statistics across these datasets.
\item \textbf{PAMAP} \cite{PAMAP} --- just PAMAP, not PAMAP2
\item \textbf{MSRC-12} \cite{msrc} --- Some description of this; maybe use this
\item \textbf{WARD} \cite{ward} --- Berkeley Ward Dataset; maybe use this
\item \textbf{ECG} \cite{physiobank} --- Some big ECG dataset from physiobank.
\item \textbf{AMPDs} \cite{ampds} --- The Almanac of Minutely Power Datasets describe electricity, water, and natural gas consumption recorded once per minute for two years at a single home. We treat the data from each of these modalities as one dataset and report aggregate performance across all three.
\end{itemize}

TODO maybe split ampds into 3 different datasets and report on each separately.

For all datasets, we separate the first 5\% of the data to use as a training set. For datasets that were not stored as integers, we quantized them such the largest and smallest values observed corresponded to the largest and smallest values representable with the number of bits tested. Note that this is the worst case scenario for our method since it makes the deltas maximally likely to overflow.

For multivariate datasets, we concatenated the data from each variable to obtain a univariate time series. As discussed previously, one might be able to obtain better performance by jointly compressing the variables, but we defer this to future work since it both makes direct comparison to existing methods more difficult and complicates the algorithm.


% ------------------------------------------------
\subsection{Comparison Algorithms}
% ------------------------------------------------

\begin{itemize}[leftmargin=4mm]
% \item \textbf{Wishful Thinking} \cite{batman} --- We just really hope that the data gets compressed and assume that it will.
% \item \textbf{Encouragement} \cite{barney} --- We foster a positive environment and allow the data to compress itself when it feels ready.
\item \textbf{FLAC} \cite{flac} --- A modern lossless audio codec.
\item \textbf{FastPFOR} \cite{fastPFOR} --- The fastest known method of compressing 32b integers.
\item \textbf{Zstd} \cite{Zstd} --- A state-of-the-art general purpose compression algorithm featuring LZ77-like compression along with a recent entropy coder \cite{fbe} based on Abstract Numeral Systems \cite{ans}.
\item \textbf{LZ4} \cite{lz4} --- A widely-used general-purpose compression algorithm optimized for speed and based on LZ77.
\item \textbf{LZ4-HC} \cite{lz4} --- A variant of LZ4 optimized for compression ratio at the cost of compression speed.
\item \textbf{GZIP} \cite{gzip} --- A widely used general-purpose compression algorithm.
\item \textbf{BitShuf} \cite{blosc} --- Blosc-LZ with the BitShuffle preprocessor, which groups runs of 0 bits when consecutive values are similar and small.
% \item \textbf{Delta+BitShuf} \cite{gzip} --- Like BitShuf, but applied to the delta-encoded representation of the time series.
% \item \textbf{Delta} --- Simple delta encoding followed by...erm...some kind of bit packing or something.
% \item \textbf{DeltaDelta} --- Delta encoding the delta encoding, as done in some popular time series databases \cite{something, influxDB}.
\end{itemize}

We also assess the above methods when applied to the delta-encoded representation of the time series, as well as the double-delta-encoded representation. We do not do this for FLAC and FastPFOR since they have similar preprocessing steps built in.

Note that all of the above except FastPFOR, and possibly FLAC with special configuration, require tens of KB, or even tens of MB, of memory, and therefore are unsuitable for many low-power devices.

% ------------------------------------------------
\subsection{Compression Speed}
% ------------------------------------------------

Look, a bar graph. Probably 4 rows x 2 cols of subplots, with row corresponding to one dataset and cols corresponding to {8b, 16b}. Each algorithm gets one bar in each subplot. Ideally, run everything a few times and show standard deviations or error bars. Maybe actually have

% \begin{figure}[h]
% \begin{center}
% % \includegraphics[width=\linewidth, trim={0 1cm 0 0},clip]{moose0}
% \includegraphics[width=\linewidth]{encoding_speed}
% \vspace*{-2mm}
% % \caption{Bolt encodes data vectors significantly faster than existing algorithms.}
% \caption{Bolt encodes both data and query vectors significantly faster than similar algorithms.}
% \label{fig:encoding_speeds}
% \end{center}
% \end{figure}

% \vspace{-2mm}
% ------------------------------------------------
\subsection{Decompression Speed}
% ------------------------------------------------

Yep. Real fast.

Same kind of layout as compression speed.

% ------------------------------------------------
\subsection{Compression Ratio}
% ------------------------------------------------

Such small. Very compress.

Same kind of layout as speed plots. Probably have higher be better for consistency.

% ------------------------------------------------
\subsection{Other Findings}
% ------------------------------------------------

We encountered a number of counter-intuitive findings regarding what does and does not improve compression ratio. In the interest of facilitating future research in this area, we briefly describe several of them here.

1) Ordering residuals by relative frequency. Absolute value correlates almost perfectly with relative frequency. % Since we'll plot it anyway, point out that residuals usually aren't Laplace distro; more like a power law (in constrast findings in \cite{shorten} for music). Or maybe make that its own bullet.

2) Residuals are heavier-tailed than a Laplace distribution, but less heavy-tailed than a power law (in constrast findings in \cite{shorten} for music).

3) Nearest-neighbor search. Helped but not worth the bit cost to provide the neighbor index; true for blocks of size 8, 16, 32. (delta then nn) is much bettern than (nn then delta). This is especially interesting given how many motif discovery papers use compression as a heuristic for finding repeating patterns; the disconnect is that they don't include the cost of specifying that a discovered pattern is present.

