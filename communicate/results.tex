
To assess \mine's effectiveness, we implemented both it and comparison algorithms in C++. All of our code and raw results are publicly available on the \mine website.\footnote{https://github.com/dblalock/sprintz} This website also contains experiments on additional datasets, as well as thorough documentation of both our code and experimental setups. All experiments use a single thread on a 2013 Macbook Pro with a 2.6GHz Intel Core i7-4960HQ processor.

All reported timings and throughputs are the best of 5 runs. We use the best, rather than average, since this is standard practice in performance benchmarking.

% ------------------------------------------------
\subsection{Datasets}
% ------------------------------------------------

For assessing accuracy, we use several publicly available datasets:
\begin{itemize}[leftmargin=4mm]
\item \textbf{UCR} \cite{ucrTimeSeries} --- The UCR Time Series Archive is a repository of 85 univariate time series datasets from various domains, commonly used for benchmarking time series algorithms. Because each dataset consists of many time series, we concat the first 100 time series from each dataset to from a single longer time series. This is to allow dictionary-based methods to share information across time series (instead of compressing each in isolation).
% Because the file format is delimited text with labels interspersed with data, we extract ``raw'' data by reading the time series within each dataset into a contiguous array of the appropriate data type We concatenated the first 100 examples from each of the 85 time series datasets in the UCR Time Series Archive \cite{ucrTimeSeries} to form 85 longer time series. Before concatenating, we subtracted off the mean from each example and interpolated one sample between its end and the start of the next example to avoid sudden jumps. This processing makes the datasets in some sense synthetic, but the result is an easy-to-reproduce benchmark incorporating time series from dozens of domains. We report aggregate statistics across these datasets.
\item \textbf{PAMAP} \cite{pamap} --- just PAMAP, not PAMAP2
\item \textbf{MSRC-12} \cite{msrc} --- Some description of this; maybe use this
\item \textbf{WARD} \cite{ward} --- Berkeley Ward Dataset; maybe use this
\item \textbf{ECG} \cite{physiobank} --- Some big ECG dataset from physiobank.
\item \textbf{AMPDs} \cite{ampds} --- The Almanac of Minutely Power Datasets describe electricity, water, and natural gas consumption recorded once per minute for two years at a single home. We treat the data from each of these modalities as one dataset and report aggregate performance across all three.
\end{itemize}

TODO maybe split ampds into 3 different datasets and report on each separately.

For datasets stored as delimited files, we first parsed the data into a contiguous, numeric array and then dumped the bytes as a binary file. For datasets that were not integer-valued, we quantized them such that the largest and smallest values observed corresponded to the largest and smallest values representable with the number of bits tested. Note that this is the worst case scenario for our method since it makes the maximizes the number of bits required to represent the data.

For multivariate datasets, we allowed all methods but our own to operate on the data one variable at a time; i.e., instead of interleaving values for every variable, we store all values for each variable contiguously. This corresponds to allowing them an unlimited buffer size in which to store incoming data before compressing it.

% For multivariate datasets, we concatenated the data from each variable to obtain a univariate time series. As discussed previously, one might be able to obtain better performance by jointly compressing the variables, but we defer this to future work since it both makes direct comparison to existing methods more difficult and complicates the algorithm.


% ------------------------------------------------
\subsection{Comparison Algorithms}
% ------------------------------------------------

\begin{itemize}[leftmargin=4mm]
\item \textbf{SIMD-BP128} \cite{fastpfor} --- The fastest known method of compressing integers.
\item \textbf{FastPFOR} \cite{fastpfor} --- An algorithm similar to SIMD-BP128, but with better compression.
\item \textbf{Simple8b} \cite{simple8b} --- An integer algorithm compression algorithm used by InfluxDB \cite{influxDB}.
\item \textbf{LZO} \cite{lzo} --- A stable and widely-used dictionary compressor employed by KairosDB \cite{kairosDB} and LittleTable \cite{littleTable}.
\item \textbf{Snappy} \cite{snappy} --- A general-purpose compression algorithm developed by Google and used by InfluxDB, KairosDB, OpenTSDB \cite{openTSDB}, RocksDB \cite{rocksDB}, the Hadoop Distributed File System \cite{hdfs} and numerous other projects.
\item \textbf{Zstd} \cite{zstd} --- Facebook's state-of-the-art general purpose compression algorithm. It is based on LZ77 and entropy codes using a mix of Huffman coding and Finite State Entropy (FSE) \cite{fse}. It is available in RocksDB \cite{rocksDB}.
\item \textbf{Brotli} \cite{brotli} --- A recent compression algorithm introduced by Google and now standardized as a web content-encoding type.
\item \textbf{LZ4} \cite{lz4} --- A widely-used general-purpose compression algorithm optimized for speed and based on LZ77. Used by RocksDB and ChronicleDB \cite{chronicleDB}.
% \item \textbf{LZ4-HC} \cite{lz4} --- A variant of LZ4 optimized for compression ratio at the cost of compression speed.
\item \textbf{DEFLATE} [] --- The compression algorithm underlying zlib \cite{zlib} and gzip \cite{gzip}. Used by RespawnDB \cite{respawnDB}, the Parquet columnar storage format \cite{parquet}, and HDFS \cite{hdfs}. We use the zlib implementation.
% \item \textbf{BitShuf} \cite{bitshuf} --- LZ4 with the BitShuffle preprocessor, which groups runs of 0 bits when consecutive values are similar and small.
% \item \textbf{Delta+BitShuf} \cite{gzip} --- Like BitShuf, but applied to the delta-encoded representation of the time series.
% \item \textbf{Delta} --- Simple delta encoding followed by...erm...some kind of bit packing or something.
% \item \textbf{DeltaDelta} --- Delta encoding the delta encoding, as done in some popular time series databases \cite{something, influxDB}.
\end{itemize}

% We also assess the above methods when applied to the delta-encoded representation of the time series, as well as the double-delta-encoded representation. We do not do this for FLAC and FastPFOR since they have similar preprocessing steps built in.

% Note that all of the above except FastPFOR, and possibly FLAC with special configuration, require tens of KB, or even tens of MB, of memory, and therefore are unsuitable for many low-power devices.

% ------------------------------------------------
\subsection{Compression Ratio}
% ------------------------------------------------

In order to rigorously assess the compression performance of both \minesp and existing algorithms, it is desirable to evaluate on a large corpus of time series from heterogeneous domains. The clear choice for such a corpus is the UCR Time Series Archive \cite{ucrTimeSeries}, which is almost universally used in the data mining community for evaluating time series classification and clustering algorithms.

Moreover, in order to avoid biasing the results, it is important to choose an appropriate overall metric. One obvious choice would be to simply measure the total size of all compressed datasets compared to the original size. Unfortunately, this would cause a small number of large datasets to dominate. Even if we took a fixed number of time series per dataset, datasets with longer time series would still contribute most of the data.

Another option would be to compute the compression ratio for each dataset and average these numbers. This is also undesirable for similar reasons. Specifically, it allows performance on a small number of highly compressible datasets to dominate the overall metric.

Because these considerations exactly parallel those associated with comparing classifiers across multiple datasets, we use the methodology outlined in \cite{cdDiagrams}. This means computing the rank of each algorithm for each dataset and comparing the mean ranks using a Nemenyi test. A rank of 1 indicates the best ratio, while 2 indicates the second-best ratio, and so on.

Results using this methodology are shown in Figure~\ref{fig:ratioCD}. Sprintz on high compression settings is significantly better than any existing algorithm. On slightly lower settings, it is still as effective as the best current methods (Zlib and Zstd).

\begin{figure}[h]
\begin{center}
    \includegraphics[width=\linewidth]{paper/cd_diagram_8b_deltas=0}
    \includegraphics[width=\linewidth]{paper/cd_diagram_16b_deltas=0}
    \caption{Compression performance of different algorithms on the UCR Time Series Archive, as measured by mean rank. Lower ranks are better. Methods joined with a horizontal black line are not statistically significantly different.}
    \label{fig:ratioCD}
\end{center}
\end{figure}

As a more intuitive illustration, we also include the distributions of raw compression ratios (Figure~\ref{fig:ratioBox}). \minesp exhibits consistently strong performance across almost all datasets. High-speed codecs such as Snappy, LZ4, and the integer codecs (FastPFOR, SIMDBP128, Simple8B) hardely compress most datasets at all.

Perhaps counter-intuitively, 8-bit data tends to yield higher compression ratios than 16-bit data. This is a product of the fact that the number of bits that are ``predictable'' is roughly constant. I.e., suppose that an algorithm can correctly predict the four most significant bits of a given value; this enables a 2:1 compression ratio in the 8-bit case, but only a 16:12 = 4:3 ratio in the 16-bit case. A more concise explanation is that all but the top few bits are difficult to distinguish from noise, so the larger the fraction of the value these bits constitute, the greater the compressibility.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=\linewidth]{paper/boxplot_ucr_deltas=0}
    \caption{Boxplots of compression performance of different algorithms on the UCR Time Series Archive. Each boxplot captures the distribution of one algorithm across all 85 datasets.}
    \label{fig:ratioBox}
\end{center}
\end{figure}

% Using the 85 UCR datasets:
% \begin{enumerate}
% \item CD Diagram of \mine (all levels) vs other algorithms
% \item CD Diagram of \mine (all levels) vs delta coding + other algorithms
% \item If we don't win in at least the first one, CD Diagram of \mine (all levels) vs delta coding + other algorithms that use very little memory
% \end{enumerate}

% % ------------------------------------------------
% \subsection{Ratio-Speed Tradeoff}
% % ------------------------------------------------

% A natural question is whether the above compression ratios come at the price of reduced speed. To test this, we recorded the speeds and compression ratios of both our method and others on several multivariate datasets. We do not simply reuse the UCR datasets because they are all univariate, which is both not our algorithm's focus and its worst case.


% ------------------------------------------------
\subsection{Decompression Speed}
% ------------------------------------------------

To systematically assess the speed of \minesp, we ran it on datasets with varying numbers of columns and varying levels of compressibility. Specifically, we generated two synthetic datasets: 1) 100 million random values uniformly distributed across the full range of those possible for the given bitwidth; and 2) 100 million random values using only the lower fourth of the bits. The former is incompressible, while the latter allows compression by a factor of two or more. It doesn't allow compression by a factor of four for \minesp since the independent elements make delta coding and \fire counterproductive.

We compressed each of these datasets with \minesp set to treat it as if it had 1 through 80 columns. Numbers that do not evenly divide 100 million result in \minesp memcpy-ing the trailing bytes.

As shown in Figure~\ref{fig:ndims_vs_speed}, \minesp becomes faster as the number of columns increases and as the number of columns approaches multiples of 32 for 8-bit data or 16 for 16-bit data. These values correspond to the 32B width of a SIMD register on the tested machine. These results demonstrate that there is small but consistent overheaded associated with using \fire over delta coding, but that both approaches are extremely fast. Without Huffman coding, \mine decompresses at multiple GB/s except for very few ($<$5) columns.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=\linewidth]{paper/ndims_vs_speed}
    \caption{\minesp becomes faster as the number of columns increases and as the width of each sample approaches multiples of 32B (on a machine with 32B vector registers). Its speed is insensitive to the compression ratio, as illustrated by the speeds in the top row (low compression) roughly equaling those in the bottom row (high compression).}
    \label{fig:ndims_vs_speed}
\end{center}
\end{figure}


% ------------------------------------------------
\subsection{Compression Speed}
% ------------------------------------------------

% Look, a bar graph. Probably 4 rows x 2 cols of subplots, with row corresponding to one dataset and cols corresponding to {8b, 16b}. Each algorithm gets one bar in each subplot. Ideally, run everything a few times and show standard deviations or error bars. Maybe actually have

% \begin{figure}[h]
% \begin{center}
% % \includegraphics[width=\linewidth, trim={0 1cm 0 0},clip]{moose0}
% \includegraphics[width=\linewidth]{encoding_speed}
% \vspace*{-2mm}
% % \caption{Bolt encodes data vectors significantly faster than existing algorithms.}
% \caption{Bolt encodes both data and query vectors significantly faster than similar algorithms.}
% \label{fig:encoding_speeds}
% \end{center}
% \end{figure}

% \vspace{-2mm}


% ------------------------------------------------
\subsection{Quantization Error}
% ------------------------------------------------

% Given that many time series as stored as floating point values, it is natural to wonder what

While floating point values are not the focus of our paper, a straightforward means of generalizing \minesp to floats is to first quantize the floating point data. The downside of doing this is that, because floating point numbers are not uniformly distributed along the real line, such quantization can be lossy. We therefore carried out an experiment to determine how lossy such quantization tends to be on real data.

We did this by quantizing the UCR time series datasets and measuring the noise that this introduces relative to the variance of the data. Specifically, we linearly offset and rescaled the time series in each dataset such that the minimum and maximum values in any time series correspond to $(0, 255)$ for 8-bit quantization or $(0, 65,535)$ for 16-bit quantization. We then obtained the quantized data by applying the floor function to this linearly transformed data.

To measure the error this introduces, we then inverted the linear transformation and computed the mean squared error between the original and this ``reconstructed'' data. The resulting error values for each dataset, normalized by the dataset's variance, are shown in Figure~\ref{fig:quantize_errs}. These normalized values can be understood as signal-to-noise ratio measurements, where the noise is the quantization error. As the figure illustrates, the quantization error is orders of magnitude smaller than the variance for nearly all datasets, and never worse than one order of magnitude even for 8-bit quantization.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=\linewidth]{paper/quantize_errs}
    \caption{Quantizing floating-point time series introduces error that is orders of magnitude smaller than the variance of the data. For every 10 Decibels of signal-to-noise ratio, the variance is 10 times larger than the quantization error. Even with 8 bits, quantization introduces less than 1\% error on nearly all datasets.}
    \label{fig:quantize_errs}
\end{center}
\end{figure}

This of course does not indicate that all time series can be safely quantized. Two counterexamples of which we are aware are timestamps where microsecond or nanosecond resolution matters, and GPS coordinates, where small decimal places may correspond to many meters. However, the above results suggest that quantization is a suitable means of applying \minesp to floating-point data in many applications. This is bolstered by previous work showing that quantization even to a mere six bits [] rarely harms classification accuracy, and quantizing to two or less sometimes improves it [].

% ------------------------------------------------
\subsection{Query acceleration}
% ------------------------------------------------

\begin{enumerate}
\item Sliding Mean on 8b data?
\item Max on 16b data? (Self-driving car acceleration, averaged over two time steps?)
\item Sliding linear classifier? Examples of action (e.g. "climbing stairs") using MSRC-12 or PAMAP? "Sliding on ice" or something in car data?
\end{enumerate}

We can also compare to Sprintz without pushing queries down into the decompress loop (ie, decompress everything first and then query) to show the benefit of our quasi-pushdown; I say ``quasi'' because, except when data gets run-length-encoded, we still do have to decompress it---just not store the decompressed output.

% ------------------------------------------------
\subsection{Other Findings}
% ------------------------------------------------

We encountered a number of counter-intuitive findings regarding what does and does not improve compression ratio. In the interest of facilitating future research in this area, we briefly describe several of them here.

1) Ordering residuals by relative frequency. Absolute value correlates almost perfectly with relative frequency. % Since we'll plot it anyway, point out that residuals usually aren't Laplace distro; more like a power law (in constrast findings in \cite{shorten} for music). Or maybe make that its own bullet.

2) Residuals are heavier-tailed than a Laplace distribution, but less heavy-tailed than a power law (in constrast findings in \cite{shorten} for music).

3) Nearest-neighbor search. Helped but not worth the bit cost to provide the neighbor index; true for blocks of size 8, 16, 32. (delta then nn) is much bettern than (nn then delta). This is especially interesting given how many motif discovery papers use compression as a heuristic for finding repeating patterns; the disconnect is that they don't include the cost of specifying that a discovered pattern is present.

